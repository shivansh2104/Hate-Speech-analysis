{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3-cqvdhifiIg"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import random\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer,TfidfTransformer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading the datasets.\n",
    "Applying label encoder and then concatenate the two datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 36426,
     "status": "ok",
     "timestamp": 1557839146475,
     "user": {
      "displayName": "Ke√ßiler Firarda",
      "photoUrl": "https://lh5.googleusercontent.com/-dUH0FLxxGuo/AAAAAAAAAAI/AAAAAAAAAB4/oXM_qmhqFsM/s64/photo.jpg",
      "userId": "11065118399992837630"
     },
     "user_tz": -180
    },
    "id": "S00HQshNBECD",
    "outputId": "84c470cf-6d0c-4e05-c50d-744f971643ef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              tweets   labels\n",
      "0  RT @Papapishu: Man it would fucking rule if we...  hateful\n",
      "1  It is time to draw close to Him &#128591;&#127...   normal\n",
      "2  if you notice me start to act different or dis...   normal\n",
      "3  Forget unfollowers, I believe in growing. 7 ne...   normal\n",
      "4  RT @Vitiligoprince: Hate Being sexually Frustr...  hateful\n",
      "5  Topped the group in TGP Disc Jam Season 2! Ont...   normal\n",
      "6  That daily baby aspirin for your #heart just m...   normal\n",
      "7  I liked a @YouTube video from @mattshea https:...   normal\n",
      "8  RT @LestuhGang_: If your fucking up &amp; your...  hateful\n",
      "9  @Move_Fwd give up. You've lost. You will not c...   normal\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df = pd.read_excel('hatespeech_text.xlsx', header = None)\n",
    "df.rename(columns={0:'tweets', 1:'labels'}, inplace=True)\n",
    "print(df.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweets</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RT @Papapishu: Man it would fucking rule if we...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>It is time to draw close to Him &amp;#128591;&amp;#127...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>if you notice me start to act different or dis...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Forget unfollowers, I believe in growing. 7 ne...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RT @Vitiligoprince: Hate Being sexually Frustr...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              tweets  labels\n",
       "0  RT @Papapishu: Man it would fucking rule if we...       0\n",
       "1  It is time to draw close to Him &#128591;&#127...       1\n",
       "2  if you notice me start to act different or dis...       1\n",
       "3  Forget unfollowers, I believe in growing. 7 ne...       1\n",
       "4  RT @Vitiligoprince: Hate Being sexually Frustr...       0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le=LabelEncoder()\n",
    "df['labels']=le.fit_transform(df['labels'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweets</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>lmfaoo  üò≠  üò≠  üò≠  üò≠  üò≠</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i hate this feeling  üò¢</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>can't believe i just went out in this cold to ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i need a new trap house, so if you really fuck...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&lt;user&gt; so very sorry for your loss.  üíî</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              tweets  labels\n",
       "0                             lmfaoo  üò≠  üò≠  üò≠  üò≠  üò≠        0\n",
       "1                            i hate this feeling  üò¢        0\n",
       "2  can't believe i just went out in this cold to ...       0\n",
       "3  i need a new trap house, so if you really fuck...       0\n",
       "4            <user> so very sorry for your loss.  üíî        0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "de=pd.read_pickle(\"./emoji_tweets.pkl\")\n",
    "emoji = pd.DataFrame.from_dict(de)\n",
    "emoji.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.concat([df,emoji])\n",
    "data.head()\n",
    "data.tweets=data.tweets.astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pre Processing\n",
    " 1-> For hate speech detection as the dataset contains tweets along with the actual tweets some of them include the Twitter handle of people, hashtags, links to certain websites or the tweet itself\n",
    "First, we replace all tags and hashtags with space using regex (@ [^\\s] + which means replace anything which @ accompanied by anything but a space). Then we replace all the website links with a space.\n",
    "For hate speech detection as the dataset contains tweets along with the actual tweets some of them include the Twitter handle of people, hashtags, links to certain websites or the tweet itself and emoticons. We add rt to stopwords which represents retweet. First, we replace all tags and hashtags with space using regex (@ [^\\s] + which means replace anything which @ accompanied by anything but a space). Then we replace all the website links with a space.\n",
    "Then we split the sentence into words and check if the word is present in stopwords we remove it and then again join the sentence. We then replace multiple spaces with a single space, strip the sentences for extra trailing spaces. we replace nan (not a number) values with a space and finally drop any missing values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 289
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 44263,
     "status": "ok",
     "timestamp": 1557839154357,
     "user": {
      "displayName": "Ke√ßiler Firarda",
      "photoUrl": "https://lh5.googleusercontent.com/-dUH0FLxxGuo/AAAAAAAAAAI/AAAAAAAAAB4/oXM_qmhqFsM/s64/photo.jpg",
      "userId": "11065118399992837630"
     },
     "user_tz": -180
    },
    "id": "WtX0VBfBf9H1",
    "outputId": "8ef59a51-ef9d-44f7-9643-abab692ff12f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading stopwords: <urlopen error [Errno 11001]\n",
      "[nltk_data]     getaddrinfo failed>\n",
      "[nltk_data] Error loading wordnet: <urlopen error [Errno 11001]\n",
      "[nltk_data]     getaddrinfo failed>\n",
      "[nltk_data] Error loading punkt: <urlopen error [Errno 11001]\n",
      "[nltk_data]     getaddrinfo failed>\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_19644\\3359290571.py:16: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  data[\"tweets\"] = data[\"tweets\"].str.replace(' +', ' ', case=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              tweets  labels\n",
      "0    man would fucking rule party perpetual warfare.       0\n",
      "1      time draw close & father, draw near always ‚ù§Ô∏è       1\n",
      "2  notice start act different distant.. bc peeped...       1\n",
      "3  forget unfollowers, believe growing. 7 new fol...       1\n",
      "4  hate sexually frustrated like wanna fuck ion w...       0\n",
      "5  topped group tgp disc jam season 2! onto semi-...       1\n",
      "6     daily baby aspirin might preventing colon too.       1\n",
      "7  liked video blue army coming! - ancient warfare 2       1\n",
      "8  fucking &amp; homies dont tell fucking up, ain...       0\n",
      "9  give up. lost. convince one iota read conspira...       1\n",
      "                                                  tweets  labels\n",
      "13190  clear message. use helmet save life üëá \\n\\n<use...       1\n",
      "13191                         funny üòÜ wish could get one       1\n",
      "13192                        show respect, get respect ‚òù       1\n",
      "13193                              waahh im happy you! üíñ       1\n",
      "13194                               beautiful gorgeous üòò       1\n",
      "13195         love waking skinny ahaha wish lasted day üòÖ       1\n",
      "13196              magnificent pair tits üòç cock hard üçÜ üòÄ       1\n",
      "13197            soon mamsh üòò god give best among best üíñ       1\n",
      "13198                                          trust u üòé       1\n",
      "13199                                       aww thanks üòÅ       1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "85966"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "STOPWORDS = stopwords.words('english')\n",
    "STOPWORDS.append(\"rt\")\n",
    "STOPWORDS.append(\"<user>\")\n",
    "STOPWORDS.append(\"<url>\")\n",
    "def clean_text():\n",
    "    data[\"tweets\"] = data[\"tweets\"].apply(lambda x: x.lower())\n",
    "    data[\"tweets\"] = [re.sub('(@[^\\s]+)|(#[^\\s]+)', '', tweet) for tweet in data[\"tweets\"]]\n",
    "    data[\"tweets\"] = [re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))','',tweet) for tweet in data[\"tweets\"]]\n",
    "    data[\"tweets\"] = data[\"tweets\"].str.split(' ').apply(lambda tweet: ' '.join(k for k in tweet if k not in STOPWORDS))\n",
    "    data[\"tweets\"] = data[\"tweets\"].str.replace(' +', ' ', case=False)\n",
    "    data[\"tweets\"] = data[\"tweets\"].str.strip()\n",
    "    data[\"tweets\"].replace('', np.nan, inplace=True)\n",
    "    df.dropna(subset=[\"tweets\"], inplace=True)\n",
    "      \n",
    "clean_text()\n",
    "    \n",
    "print(data.head(10))\n",
    "print(data.tail(10))\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For emoticon function first we tokenize each word of the sentence using word_tokenize. Then we check whether the word is a emoji or not if no we add if to sentence else we convert the emoji into its corresponding meaning (ie üòÄ turns to smile) using the emoji.demojize function imported from library emoji , convert the customary ':' sign attached to it to a space, remove extra spaces and then add it to the sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13195    love waking skinny ahaha wish lasted day  grin...\n",
       "13196    magnificent pair tits  smiling_face_with_heart...\n",
       "13197    soon mamsh  face_blowing_a_kiss  god give best...\n",
       "13198              trust u  smiling_face_with_sunglasses  \n",
       "13199         aww thanks  beaming_face_with_smiling_eyes  \n",
       "Name: tweets, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import emoji\n",
    "def emoticon(sentence):\n",
    "    words=word_tokenize(sentence)\n",
    "    stem_sentence=[]\n",
    "    for word in words:\n",
    "        if emoji.demojize(word)== None:\n",
    "            stem_sentence.append(word)\n",
    "            stem_sentence.append(\" \")\n",
    "        else:\n",
    "            word= emoji.demojize(word)\n",
    "            word = word.replace(\":\",\" \")\n",
    "            stem_sentence.append(word)\n",
    "            stem_sentence.append(\" \")\n",
    "    return \"\".join(stem_sentence)\n",
    "    \n",
    "training_size = 1000000        \n",
    "sentences = data.tweets.astype(str)\n",
    "sentences = sentences[:training_size].apply(emoticon)\n",
    "sentences.head()\n",
    "sentences.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: emoji in c:\\users\\hp\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (1.7.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.1.1; however, version 22.0.4 is available.\n",
      "You should consider upgrading via the 'c:\\users\\hp\\appdata\\local\\programs\\python\\python38\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install emoji"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we convert our words into root words after analyzing its context in the sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13195    love waking skinny ahaha wish lasted day grinn...\n",
       "13196    magnificent pair tit smiling_face_with_heart-e...\n",
       "13197    soon mamsh face_blowing_a_kiss god give best a...\n",
       "13198                trust u smiling_face_with_sunglasses \n",
       "13199           aww thanks beaming_face_with_smiling_eyes \n",
       "Name: tweets, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.stem import LancasterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "def stemSentence(sentence):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    stemmer = LancasterStemmer()\n",
    "    words=word_tokenize(sentence)\n",
    "    stem_sentence=[]\n",
    "    for word in words:\n",
    "        stem_sentence.append(lemmatizer.lemmatize(word))\n",
    "        stem_sentence.append(\" \")\n",
    "    return \"\".join(stem_sentence)\n",
    "training_size = 1000000\n",
    "sentences = sentences[:training_size].apply(stemSentence)\n",
    "sentences.head()\n",
    "sentences.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting the dataset into 70:30 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 44497,
     "status": "ok",
     "timestamp": 1557839154638,
     "user": {
      "displayName": "Ke√ßiler Firarda",
      "photoUrl": "https://lh5.googleusercontent.com/-dUH0FLxxGuo/AAAAAAAAAAI/AAAAAAAAAB4/oXM_qmhqFsM/s64/photo.jpg",
      "userId": "11065118399992837630"
     },
     "user_tz": -180
    },
    "id": "lvlivu42gDDg",
    "outputId": "ef36d3b0-42be-42e5-fac9-b2169212f6cf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total   tweet count: 99166\n",
      "Normal  tweet count: 60450\n",
      "Hateful tweet count: 38716 \n",
      "\n",
      "Total tweet count in training sampple: 69416\n",
      "Total tweet count in test sample:     29750\n",
      "Normal  tweet count in X_train: 42344\n",
      "Hateful tweet count in X_train: 27072\n",
      "Normal  tweet count in X_test:  18106\n",
      "Hateful tweet count in X_test:  11644\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Data splitting\n",
    "X = sentences\n",
    "y = data[\"labels\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state = 42)\n",
    "\n",
    "print ('Total   tweet count:', len(data))\n",
    "print ('Normal  tweet count:', len(data[data.labels == 1]))\n",
    "print ('Hateful tweet count:', len(data[data.labels == 0]), '\\n')\n",
    "print ('Total tweet count in training sampple:', len(X_train))\n",
    "print ('Total tweet count in test sample:    ', len(X_test))\n",
    "print ('Normal  tweet count in X_train:', X_train[y_train == 1].count())\n",
    "print ('Hateful tweet count in X_train:', X_train[y_train == 0].count())\n",
    "print ('Normal  tweet count in X_test: ', X_test[y_test == 1].count())\n",
    "print ('Hateful tweet count in X_test: ', X_test[y_test == 0].count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fitting and transforming tfidfvectorizer on training sentences and transforming testing sentences based on training sentences\n",
    "Create a pickle file containing tfidf fitted vectorizer so that we can call in our python file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1379,
     "status": "ok",
     "timestamp": 1557839658880,
     "user": {
      "displayName": "Ke√ßiler Firarda",
      "photoUrl": "https://lh5.googleusercontent.com/-dUH0FLxxGuo/AAAAAAAAAAI/AAAAAAAAAB4/oXM_qmhqFsM/s64/photo.jpg",
      "userId": "11065118399992837630"
     },
     "user_tz": -180
    },
    "id": "T9n-slF7dTus",
    "outputId": "9fc437ba-39b4-4daa-97f1-f7eda0ed1cb9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(69416, 49384)\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "cv = CountVectorizer()\n",
    "tfidftrans = TfidfVectorizer()\n",
    "X_train = tfidftrans.fit_transform(X_train)\n",
    "print(X_train.shape)\n",
    "pickle.dump(tfidftrans, open(\"tfidf.pickle\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(29750, 49384)\n"
     ]
    }
   ],
   "source": [
    "X_test = tfidftrans.transform(X_test)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feeding the data to MultinomialNB, KNN ,Logistic regression, Decision Tree\n",
    "Adding the accuracy scores to a dictionary to compare later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8883025210084033\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.83      0.85     11644\n",
      "           1       0.89      0.93      0.91     18106\n",
      "\n",
      "    accuracy                           0.89     29750\n",
      "   macro avg       0.89      0.88      0.88     29750\n",
      "weighted avg       0.89      0.89      0.89     29750\n",
      "\n",
      "[[ 9664  1980]\n",
      " [ 1343 16763]]\n"
     ]
    }
   ],
   "source": [
    "def training(clf,x_train,Y_train):\n",
    "    clf.fit(x_train,Y_train)\n",
    "def predict(clf,X_test):\n",
    "    return clf.predict(X_test)\n",
    "dict={}\n",
    "from sklearn.metrics import confusion_matrix,accuracy_score,roc_curve,classification_report\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "mnb = MultinomialNB(alpha =0.2)\n",
    "training(mnb,X_train,y_train)\n",
    "pred = predict(mnb,X_test)\n",
    "print(accuracy_score(y_test,pred,normalize=True))\n",
    "print(classification_report(y_test, pred))\n",
    "print(confusion_matrix(y_test,pred))\n",
    "dict[mnb]=accuracy_score(y_test,pred,normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k =  1\n",
      "0.7767394957983194\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.48      0.63     11644\n",
      "           1       0.74      0.97      0.84     18106\n",
      "\n",
      "    accuracy                           0.78     29750\n",
      "   macro avg       0.82      0.72      0.73     29750\n",
      "weighted avg       0.80      0.78      0.76     29750\n",
      "\n",
      "[[ 5632  6012]\n",
      " [  630 17476]]\n",
      "k =  2\n",
      "0.776235294117647\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.50      0.63     11644\n",
      "           1       0.75      0.96      0.84     18106\n",
      "\n",
      "    accuracy                           0.78     29750\n",
      "   macro avg       0.81      0.73      0.74     29750\n",
      "weighted avg       0.80      0.78      0.76     29750\n",
      "\n",
      "[[ 5769  5875]\n",
      " [  782 17324]]\n",
      "k =  3\n",
      "0.7519663865546219\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.39      0.55     11644\n",
      "           1       0.72      0.98      0.83     18106\n",
      "\n",
      "    accuracy                           0.75     29750\n",
      "   macro avg       0.83      0.69      0.69     29750\n",
      "weighted avg       0.80      0.75      0.72     29750\n",
      "\n",
      "[[ 4572  7072]\n",
      " [  307 17799]]\n",
      "k =  4\n",
      "0.7589915966386555\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.42      0.58     11644\n",
      "           1       0.72      0.97      0.83     18106\n",
      "\n",
      "    accuracy                           0.76     29750\n",
      "   macro avg       0.82      0.70      0.71     29750\n",
      "weighted avg       0.80      0.76      0.73     29750\n",
      "\n",
      "[[ 4940  6704]\n",
      " [  466 17640]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "for i in range(1,5):\n",
    "    knn = KNeighborsClassifier(n_neighbors=i)\n",
    "    training(knn,X_train,y_train)\n",
    "    pred = predict(knn,X_test)\n",
    "    print('k = ',i)\n",
    "    print(accuracy_score(y_test,pred,normalize=True))\n",
    "    print(classification_report(y_test, pred))\n",
    "    print(confusion_matrix(y_test,pred))\n",
    "    dict[knn,i]=accuracy_score(y_test,pred,normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.916\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.90      0.89     11644\n",
      "           1       0.94      0.93      0.93     18106\n",
      "\n",
      "    accuracy                           0.92     29750\n",
      "   macro avg       0.91      0.91      0.91     29750\n",
      "weighted avg       0.92      0.92      0.92     29750\n",
      "\n",
      "[[10501  1143]\n",
      " [ 1356 16750]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "dt = DecisionTreeClassifier()\n",
    "training(dt,X_train,y_train)\n",
    "pred = predict(dt,X_test)\n",
    "print(accuracy_score(y_test,pred,normalize=True))\n",
    "print(classification_report(y_test, pred))\n",
    "print(confusion_matrix(y_test,pred))\n",
    "dict[dt]=accuracy_score(y_test,pred,normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9355294117647058\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.88      0.91     11644\n",
      "           1       0.93      0.97      0.95     18106\n",
      "\n",
      "    accuracy                           0.94     29750\n",
      "   macro avg       0.94      0.93      0.93     29750\n",
      "weighted avg       0.94      0.94      0.94     29750\n",
      "\n",
      "[[10250  1394]\n",
      " [  524 17582]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "lr = LogisticRegression(max_iter=10000)\n",
    "training(lr,X_train,y_train)\n",
    "pred = predict(lr,X_test)\n",
    "print(accuracy_score(y_test,pred,normalize=True))\n",
    "print(classification_report(y_test, pred))\n",
    "print(confusion_matrix(y_test,pred))\n",
    "dict[lr]=accuracy_score(y_test,pred,normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding model with highest accuracy and creating a pickle file of respective model to call in python file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9355294117647058\n",
      "LogisticRegression(max_iter=10000)\n"
     ]
    }
   ],
   "source": [
    "n=max(dict.values())\n",
    "print(n)\n",
    "for name,predicted in dict.items():\n",
    "    if predicted==n:\n",
    "        print(name)\n",
    "        model=name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\numpy\\lib\\stride_tricks.py:341: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  array = np.array(array, copy=False, subok=subok)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "names = list(dict.keys())\n",
    "values = list(dict.values())\n",
    "plt.barh(range(len(dict)), values, tick_label=names)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('hate_speech.pickle', 'wb') as handle:\n",
    "    pickle.dump(model, handle, protocol=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TESTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence: hello how are u\n"
     ]
    }
   ],
   "source": [
    "sentence = input(\"sentence: \")\n",
    "def clean_text(sentence):\n",
    "    sentence = sentence.lower()\n",
    "    sentence = re.sub('(@[^\\s]+)|(#[^\\s]+)', '', sentence)\n",
    "    sentence = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))','',sentence)\n",
    "    sentence = re.sub(r'[ ]+', ' ', sentence)\n",
    "    words = word_tokenize(sentence)\n",
    "    words = [word for word in words if not word in STOPWORDS]\n",
    "    clean_sentence=[]\n",
    "    for word in words:\n",
    "        clean_sentence.append(\" \")\n",
    "    return \"\".join(clean_sentence)\n",
    "clean_text(sentence)\n",
    "sentence=emoticon(sentence)\n",
    "stemmed = stemSentence(sentence)\n",
    "sentence=[stemmed]\n",
    "sentence = tfidftrans.transform(sentence)\n",
    "#print('normal' if lr.predict(sentence)==1 else 'hateful')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 10000\n",
    "embedding_dim = 16\n",
    "max_length = 200\n",
    "trunc_type='post'\n",
    "padding_type='post' \n",
    "oov_tok = \"<OOV>\"\n",
    "training_size = 100000\n",
    "total_size = 1000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_sentences, testing_sentences, training_labels, testing_labels = train_test_split(X,y, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "tokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_tok)\n",
    "tokenizer.fit_on_texts(training_sentences)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "training_sequences = tokenizer.texts_to_sequences(training_sentences)\n",
    "training_padded = pad_sequences(training_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
    "\n",
    "testing_sequences = tokenizer.texts_to_sequences(testing_sentences)\n",
    "testing_padded = pad_sequences(testing_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import LSTM\n",
    "import tensorflow as tf\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
    "    tf.keras.layers.Bidirectional(LSTM(9)),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=3,restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "2480/2480 [==============================] - 195s 78ms/step - loss: 0.2771 - accuracy: 0.9091 - val_loss: 0.1884 - val_accuracy: 0.9396\n",
      "Epoch 2/20\n",
      "2480/2480 [==============================] - 195s 78ms/step - loss: 0.1841 - accuracy: 0.9467 - val_loss: 0.1875 - val_accuracy: 0.9414\n",
      "Epoch 3/20\n",
      "2480/2480 [==============================] - 198s 80ms/step - loss: 0.1646 - accuracy: 0.9519 - val_loss: 0.1929 - val_accuracy: 0.9409\n",
      "Epoch 4/20\n",
      "2480/2480 [==============================] - 226s 91ms/step - loss: 0.1488 - accuracy: 0.9557 - val_loss: 0.2094 - val_accuracy: 0.9380\n",
      "Epoch 5/20\n",
      "2480/2480 [==============================] - 223s 90ms/step - loss: 0.1361 - accuracy: 0.9592 - val_loss: 0.2224 - val_accuracy: 0.9370\n",
      "Epoch 6/20\n",
      "2480/2480 [==============================] - 223s 90ms/step - loss: 0.1260 - accuracy: 0.9621 - val_loss: 0.2402 - val_accuracy: 0.9346\n",
      "Epoch 7/20\n",
      "2480/2480 [==============================] - 222s 90ms/step - loss: 0.1142 - accuracy: 0.9651 - val_loss: 0.2591 - val_accuracy: 0.9311\n",
      "Epoch 8/20\n",
      "2480/2480 [==============================] - 223s 90ms/step - loss: 0.1063 - accuracy: 0.9670 - val_loss: 0.2876 - val_accuracy: 0.9308\n",
      "Epoch 9/20\n",
      "2480/2480 [==============================] - 224s 90ms/step - loss: 0.0985 - accuracy: 0.9697 - val_loss: 0.3091 - val_accuracy: 0.9301\n",
      "Epoch 10/20\n",
      "2480/2480 [==============================] - 218s 88ms/step - loss: 0.0914 - accuracy: 0.9716 - val_loss: 0.3180 - val_accuracy: 0.9261\n",
      "Epoch 11/20\n",
      "2480/2480 [==============================] - 213s 86ms/step - loss: 0.0854 - accuracy: 0.9738 - val_loss: 0.3566 - val_accuracy: 0.9215\n",
      "Epoch 12/20\n",
      "2480/2480 [==============================] - 216s 87ms/step - loss: 0.0813 - accuracy: 0.9751 - val_loss: 0.3458 - val_accuracy: 0.9247\n",
      "Epoch 13/20\n",
      "2480/2480 [==============================] - 2539s 1s/step - loss: 0.0744 - accuracy: 0.9772 - val_loss: 0.3827 - val_accuracy: 0.9220\n",
      "Epoch 14/20\n",
      "2480/2480 [==============================] - 195s 78ms/step - loss: 0.0679 - accuracy: 0.9792 - val_loss: 0.4068 - val_accuracy: 0.9216\n",
      "Epoch 15/20\n",
      "2480/2480 [==============================] - 199s 80ms/step - loss: 0.0652 - accuracy: 0.9802 - val_loss: 0.4410 - val_accuracy: 0.9218\n",
      "Epoch 16/20\n",
      "2480/2480 [==============================] - 188s 76ms/step - loss: 0.0638 - accuracy: 0.9807 - val_loss: 0.4380 - val_accuracy: 0.9227\n",
      "Epoch 17/20\n",
      "2480/2480 [==============================] - 195s 79ms/step - loss: 0.0591 - accuracy: 0.9818 - val_loss: 0.4893 - val_accuracy: 0.9162\n",
      "Epoch 18/20\n",
      "2480/2480 [==============================] - 198s 80ms/step - loss: 0.0564 - accuracy: 0.9833 - val_loss: 0.4844 - val_accuracy: 0.9219\n",
      "Epoch 19/20\n",
      "2480/2480 [==============================] - 191s 77ms/step - loss: 0.0535 - accuracy: 0.9837 - val_loss: 0.5050 - val_accuracy: 0.9210\n",
      "Epoch 20/20\n",
      "2480/2480 [==============================] - 196s 79ms/step - loss: 0.0534 - accuracy: 0.9843 - val_loss: 0.5102 - val_accuracy: 0.9163\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(training_padded, training_labels, \n",
    "                    epochs=20, \n",
    "                    validation_data=(testing_padded, testing_labels), \n",
    "                    verbose=1,callbacks=[callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_4_layer_call_fn, lstm_cell_4_layer_call_and_return_conditional_losses, lstm_cell_5_layer_call_fn, lstm_cell_5_layer_call_and_return_conditional_losses while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: lstm\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: lstm\\assets\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x0000024AFBA6D1F0> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x0000024AFBA6DA90> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
     ]
    }
   ],
   "source": [
    "model.save('lstm')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "classifier= keras.models.load_model('lstm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Model.summary of <keras.engine.sequential.Sequential object at 0x000001BD6ED116D0>>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def result(sentence):\n",
    "    def clean_text(sentence):\n",
    "        sentence = sentence.lower()\n",
    "        sentence = re.sub('(@[^\\s]+)|(#[^\\s]+)', '', sentence)\n",
    "        sentence = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))','',sentence)\n",
    "        sentence = re.sub(r'[ ]+', ' ', sentence)\n",
    "        words = word_tokenize(sentence)\n",
    "        words = [word for word in words if not word in STOPWORDS]\n",
    "        clean_sentence=[]\n",
    "        for word in words:\n",
    "            clean_sentence.append(\" \")\n",
    "        return \"\".join(clean_sentence)\n",
    "    clean_text(sentence)\n",
    "    sentence=emoticon(sentence)\n",
    "    stemmed = stemSentence(sentence)\n",
    "    sentence = tokenizer.texts_to_sequences([sentence])\n",
    "    sentence = pad_sequences(sentence, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
    "    print(\"Probability of sentence being normal ->\")\n",
    "    print(classifier.predict(sentence))\n",
    "    print(\"Hateful or normal->\")\n",
    "    print('normal' if classifier.predict(sentence)>0.5 else 'hateful')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seafood\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Tweet->\n",
      "Let‚Äôs get lost baby https://t.co/njuEUvklu1\n",
      "Probability of sentence being normal ->\n",
      "[[0.81802666]]\n",
      "Hateful or normal->\n",
      "normal\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Tweet->\n",
      "GRILLED MIXED SEAFOOD PLATTER! recipe @ https://t.co/kCrSuDBSLj https://t.co/yyUKbeJxGS\n",
      "Probability of sentence being normal ->\n",
      "[[0.9859362]]\n",
      "Hateful or normal->\n",
      "normal\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Tweet->\n",
      "Stinky man &lt;3 #fantroll #digitalart #illustration https://t.co/7bxQJfgHwE\n",
      "Probability of sentence being normal ->\n",
      "[[0.00219777]]\n",
      "Hateful or normal->\n",
      "hateful\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Tweet->\n",
      "These shrimp have tequila. These shrimp have lime. These shrimp have cilantro. What are you waiting for? Get the re‚Ä¶ https://t.co/jLdR2C0tQi\n",
      "Probability of sentence being normal ->\n",
      "[[0.99999976]]\n",
      "Hateful or normal->\n",
      "normal\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Tweet->\n",
      "Monster Humboldt squid showing up on Vancouver seafood menus as waters warm, UBC-led research finds https://t.co/W1xuGO107T\n",
      "Probability of sentence being normal ->\n",
      "[[5.627556e-06]]\n",
      "Hateful or normal->\n",
      "hateful\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Tweet->\n",
      "Seafood https://t.co/DC5Vt3mBE8\n",
      "Probability of sentence being normal ->\n",
      "[[0.99962693]]\n",
      "Hateful or normal->\n",
      "normal\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Tweet->\n",
      "OK that deviled crab sandwich looks amazing!! WOW! This is all great, @RobertIrvine ü§© #RestaurantImpossible https://t.co/DfQtcPhuyD\n",
      "Probability of sentence being normal ->\n",
      "[[0.99999845]]\n",
      "Hateful or normal->\n",
      "normal\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Tweet->\n",
      "Himst #fantroll #homestuck #illustration #digitalart https://t.co/A3WfeGaymj\n",
      "Probability of sentence being normal ->\n",
      "[[0.12758848]]\n",
      "Hateful or normal->\n",
      "hateful\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Tweet->\n",
      "Atlanta Police released surveillance video showing 51-year-old security guard‚Äôs final moments when he was shot in t‚Ä¶ https://t.co/pg3Mf7dINQ\n",
      "Probability of sentence being normal ->\n",
      "[[0.9999974]]\n",
      "Hateful or normal->\n",
      "normal\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Tweet->\n",
      "I love seafood ü¶û &amp; pork Tom yum noodle üçú https://t.co/VB2BhPRDhu\n",
      "Probability of sentence being normal ->\n",
      "[[0.99999154]]\n",
      "Hateful or normal->\n",
      "normal\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Tweet->\n",
      "today's jellycat is the sensational seafood shrimp! https://t.co/lVl1rVk9g3\n",
      "Probability of sentence being normal ->\n",
      "[[0.9971508]]\n",
      "Hateful or normal->\n",
      "normal\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Tweet->\n",
      "Seafood spot innit https://t.co/LnxzCYTvwZ\n",
      "Probability of sentence being normal ->\n",
      "[[0.9044771]]\n",
      "Hateful or normal->\n",
      "normal\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Tweet->\n",
      "It‚Äôs a pleasure to be able to tell you that the star of this week‚Äôs #CookbookCorner is The Dusty Knuckle by‚Ä¶ https://t.co/x3A4TFOTwN\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability of sentence being normal ->\n",
      "[[0.9999963]]\n",
      "Hateful or normal->\n",
      "normal\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Tweet->\n",
      "Jurong BBQ seafood stall sign says closure for a week due to chef falling out of love &amp; needing break‚Ä¶ https://t.co/RlW7ogS1CS\n",
      "Probability of sentence being normal ->\n",
      "[[9.7461656e-05]]\n",
      "Hateful or normal->\n",
      "hateful\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Tweet->\n",
      "‚ô° Sensational Seafood Lobster https://t.co/kJLwVfY6H5\n",
      "Probability of sentence being normal ->\n",
      "[[0.9997765]]\n",
      "Hateful or normal->\n",
      "normal\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Tweet->\n",
      "Chippis #shitpost #fantroll #doodles https://t.co/bTTwQVHIsX\n",
      "Probability of sentence being normal ->\n",
      "[[0.9697814]]\n",
      "Hateful or normal->\n",
      "normal\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Tweet->\n",
      "Walaupun beberapa hari lepas dah iftar makan Siakap Stim Limau, harini tetap nak makan makanan yang sama lagi hehe‚Ä¶ https://t.co/Wx5baGIAHK\n",
      "Probability of sentence being normal ->\n",
      "[[0.99964166]]\n",
      "Hateful or normal->\n",
      "normal\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Tweet->\n",
      "Not liking seafood is a serious cause for concern. The variety??\n",
      "Probability of sentence being normal ->\n",
      "[[0.99998957]]\n",
      "Hateful or normal->\n",
      "normal\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Tweet->\n",
      "Miso Butter-Seared Sea Bass with Roasted Vegetables https://t.co/XcCjhzHusE\n",
      "Probability of sentence being normal ->\n",
      "[[0.99992055]]\n",
      "Hateful or normal->\n",
      "normal\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Tweet->\n",
      "Seafood ü¶ê https://t.co/YGzatbUhHT\n",
      "Probability of sentence being normal ->\n",
      "[[0.9989175]]\n",
      "Hateful or normal->\n",
      "normal\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Tweet->\n",
      "How do you like your seafood marinara? https://t.co/KavdFvTogc\n",
      "Probability of sentence being normal ->\n",
      "[[0.99999857]]\n",
      "Hateful or normal->\n",
      "normal\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "import sys\n",
    "\n",
    "header = {\n",
    "\t'authorization': ('Bearer AAAAAAAAAAAAAAAAAAAAA'\n",
    "                        'NRILgAAAAAAnNwIzUejRCOuH5E6I8xnZ'\n",
    "                        'z4puTs%3D1Zv7ttfk8LF81IUq16cHjhLTvJu4FA33AGWWjCpTnA'),\n",
    "\t'x-twitter-client-language': 'en'\n",
    "\t}\n",
    "\n",
    "# FORGING GUEST TOKEN\n",
    "res = requests.post('https://api.twitter.com/1.1/guest/activate.json',headers=header)\n",
    "guest_token = res.json()['guest_token']\n",
    "header['x-guest-token'] = guest_token\n",
    "\n",
    "# PERFORMING SEARCH\n",
    "query = input()\n",
    "url = f'https://twitter.com/i/api/2/search/adaptive.json?simple_quoted_tweet=true&q={query}&count=20&query_source=typed_query'\n",
    "res = requests.get(url, headers=header)\n",
    "\n",
    "# PRINTING TWEETS\n",
    "for tweet in res.json().get('globalObjects').get('tweets').values():\n",
    "    print('-' * 500) \n",
    "    print(\"Tweet->\")\n",
    "    print(tweet.get('text'))\n",
    "    result(tweet.get('text'))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i love you üòä\n",
      "Probability of sentence being normal ->\n",
      "[[0.99504995]]\n",
      "Hateful or normal->\n",
      "normal\n"
     ]
    }
   ],
   "source": [
    "s=input()\n",
    "result(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "NB_SVM_LogReg_KNN_DT_RF_GB_XGB.ipynb",
   "provenance": [
    {
     "file_id": "1QA5mN7kdiFlrXEN3k_SWKbHWbPxKmgRf",
     "timestamp": 1557476569585
    }
   ],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
